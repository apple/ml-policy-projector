{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179666da-595f-4b02-830b-42513723df3a",
   "metadata": {},
   "source": [
    "## End-to-end notebook for processing Anthropic data\n",
    "\n",
    "_This notebook demonstrates how to processes Anthropic hh-rlhf data to demo Policy Maps, including over-sampling policy relevant samples for clearer policy visualization. The original dataset card can be found at: https://huggingface.co/datasets/Anthropic/hh-rlhf_ \n",
    "\n",
    "For licensing see accompanying `LICENSE` file.\n",
    "Copyright (C) 2025 Apple Inc. All Rights Reserved.\n",
    "\n",
    "⚠️ **Content Warning**: This tutorial covers the same use case from our paper: AI _safety_ policy. We use a formatted version of [hh-rlhf dataset from Anthropic](https://huggingface.co/datasets/Anthropic/hh-rlhf) from Bai et al. 2022, which is an LLM safety dataset. **This data contains harmful, unethical, and upsetting material which may be triggering to some individuals**. Please proceed with caution and mindfullness to your own wellbeing. The dataset content does not reflect the views of Apple or the authors.\n",
    "\n",
    "This notebook produces the various files needed for Policy Projector library and web visualization:\n",
    "* `<dataset>_by_concept.parquet`\n",
    "* `<dataset>_concepts.json`\n",
    "* `<dataset>_embeddings.npy`\n",
    "* `<dataset>_projection.parquet`\n",
    "* `<dataset>.csv`\n",
    "* `<dataset>.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31a2ce-87e9-467c-b045-409849930a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ce303-ce29-49c8-ae5b-76be4b3afe67",
   "metadata": {},
   "source": [
    "## 0: Parameters to specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee28a05-2a72-4195-9800-79f56c2af074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit these parameters:\n",
    "\n",
    "# Specify path to data\n",
    "DATA_DIR = \"../data/\"\n",
    "DATA_NAME = \"anthropic\"\n",
    "INPUT_COL = \"user_input\"  # Column containing input text to summarize\n",
    "OUTPUT_COL = \"model_output\"  # Column containing original output text (will be replaced)\n",
    "N_TO_GENERATE = 1000  # Number of instances to sample from the full dataset\n",
    "\n",
    "# Full data folder (not sampled)\n",
    "DATA_FOLDER = f'{DATA_DIR}/{DATA_NAME}/'\n",
    "CSV_PATH = f'{DATA_FOLDER}/{DATA_NAME}.csv'\n",
    "CONCEPT_PATH = f'{DATA_FOLDER}/{DATA_NAME}_concepts.json'\n",
    "BY_CONCEPT_PATH = f\"{DATA_FOLDER}/{DATA_NAME}_by_concept.parquet\"\n",
    "\n",
    "# Assigned name for new dataset (will be visible in folder and UI)\n",
    "CUR_DATASET = f\"{DATA_NAME}_{N_TO_GENERATE}\"\n",
    "DATA_FOLDER_SAMPLED = f'{DATA_DIR}/{CUR_DATASET}'\n",
    "CSV_PATH_SAMPLED = f'{DATA_FOLDER_SAMPLED}/{CUR_DATASET}.csv'\n",
    "PARQUET_PATH_SAMPLED = f\"{DATA_FOLDER_SAMPLED}/{CUR_DATASET}.parquet\"\n",
    "BY_CONCEPT_PATH_SAMPLED = f\"{DATA_FOLDER_SAMPLED}/{CUR_DATASET}_by_concept.parquet\"\n",
    "CONCEPT_PATH_SAMPLED = f'{DATA_FOLDER_SAMPLED}/{CUR_DATASET}_concepts.json'\n",
    "EMBEDDINGS_PATH = f\"{DATA_FOLDER_SAMPLED}/{CUR_DATASET}_embeddings.npy\"\n",
    "PROJECTIONS_PATH = f\"{DATA_FOLDER_SAMPLED}/{CUR_DATASET}_projection.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e32bf-af05-4061-9f4c-3dee5923b067",
   "metadata": {},
   "source": [
    "## 1. Download Anthropic HH-RLHF dataset\n",
    "\n",
    "This step downloads the dataset from Hugging Face, and takes the training split of the data to produce `anthropic.csv`\n",
    "\n",
    "_(Skip this step if you've already downloaded the data)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0b92e-ed64-4952-9f49-d52cfa051220",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"red-team-attempts\")\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985b278-8a74-44ad-89cf-79411d93d05d",
   "metadata": {},
   "source": [
    "Since we don't have a single model output for this dataset, we're going to pretend that the transcript is the output for the purposes of this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2e042-e0cb-4e47-8fa0-6e2074022df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model_output'] = df['transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac80a7-ca0c-4269-ae55-761266757375",
   "metadata": {},
   "source": [
    "Next we'll save the raw data to sample from later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675d490-8e40-4330-86ee-62124d557325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new folder\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "# Save dataset\n",
    "df.to_csv(CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c71beb-2f88-41c5-b7aa-4e83123fa2c1",
   "metadata": {},
   "source": [
    "## 2. Extract existing concepts from the data\n",
    "\n",
    "The Anthropic HH-RLHF red teaming dataset comes with harm labels for each red teaming attack attempt. We'll use these harm labels as the initial set of concepts in our policy map. For more on this dataset's harm labels, see Ganguli et al., 2022 https://arxiv.org/pdf/2209.07858\n",
    "\n",
    "This step produces `anthropic_concepts.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9e14e-7043-4ec0-8a56-74c7047d4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3e1f6-f4f8-48c7-8da4-1f4171cf19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'tags' # Column containing concept labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497301ad-3b0c-4a98-91f7-025c6cdc86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in unlabeled samples and turn string list into a list\n",
    "def clean_tags(x, to_str=False):\n",
    "    none = 'safe' # Concept to indicate no harm label\n",
    "    if pd.isna(x):\n",
    "        return none\n",
    "    tag_arr = x.strip('][').replace('\\n', '').split(\"' '\")\n",
    "    tag_arr = [x.replace('\\'', '') for x in tag_arr]\n",
    "    tag_arr = list(set(tag_arr))\n",
    "    if to_str:\n",
    "        return \"; \".join(tag_arr)\n",
    "    return tag_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb15f2-4224-47af-be0a-1734cde612e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tags'] = df['tags'].apply(lambda x: clean_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956e9c4-7a4f-43b6-8f13-e24e12867dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full list of concepts -> 21 concepts\n",
    "concept_list = list(df['tags'].explode().unique())\n",
    "print(len(concept_list), concept_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5dba7-3fbb-4429-b4d3-c9bf8ccf6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore 'Other', 'N/A - Invalid attempt' --> 19 concepts\n",
    "to_remove = ['Other', 'N/A - Invalid attempt']\n",
    "concept_list = [c for c in concept_list if c not in to_remove]\n",
    "print(len(concept_list), concept_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30effc24-79a7-459b-afd2-3742c31ed5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_json = [{'name': h, 'definition': '', 'examples': [], 'display_name': h} for h in concept_list]\n",
    "all_json = [{'name': 'Harm Category', 'definition': '', 'concepts': concepts_json}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170380f-d913-4dad-86d5-b06484a7fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full concept list to file\n",
    "with open(CONCEPT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_json, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae0dcb9-8b09-448f-a9cb-c9709b4cd772",
   "metadata": {},
   "source": [
    "## 3. Sort data by concept\n",
    "\n",
    "Next, we'll produce a one-hot-encoding of each sample by concept. This will make it faster to sort and filter the dataset by concept, and sample the dataset by concept. We use `parquet` since it's more efficient than `csv` to use in a web visualization with DuckDB and Mosaic (https://idl.uw.edu/mosaic/duckdb/)\n",
    "\n",
    "This step produces `anthropic_by_concept.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddc32a-e054-4ef5-ba8d-45335a89ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept = pd.DataFrame(index = df.index)\n",
    "for concept in concept_list:\n",
    "    df_concept[concept] = df['tags'].apply(lambda x: np.isin(concept, x)).astype(int)\n",
    "\n",
    "df_concept.index.names = ['rowID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eb7119-f358-4f6c-8d1c-eae14b03c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a majority \"safe\" samples in this dataset\n",
    "df_concept['safe'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc958cb-67a9-4c5d-8441-c2f89080f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are much fewer samples containing certain harms\n",
    "df_concept['Theft'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41d640-b698-4c97-8f7c-df9eeb01eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept.to_parquet(BY_CONCEPT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a26d99-f2f5-4622-8244-a27b80955024",
   "metadata": {},
   "source": [
    "## 4. Anthropic data sampling\n",
    "\n",
    "Next, we're going create a sample of the dataset. Since this demo data has a vast majority of `safe` concept samples, we're going to purposefully oversample samples with other concepts to create a dataset that has more policy-relevant cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3353fb-3870-4362-8b80-4d508a695dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new folder for sampled data\n",
    "if not os.path.exists(DATA_FOLDER_SAMPLED):\n",
    "    os.makedirs(DATA_FOLDER_SAMPLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980f39f-0c37-460d-b437-41a125b599a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy concept starter files to new folder\n",
    "! scp  {CONCEPT_PATH} {DATA_FOLDER_SAMPLED}\n",
    "! mv {DATA_FOLDER_SAMPLED}/{DATA_NAME}_concepts.json {DATA_FOLDER_SAMPLED}/{CUR_DATASET}_concepts.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e064f1-f3f3-4b56-87bf-9cfca5f984c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n samples of a given concept\n",
    "def get_sample(df_feat, concept_name, n, selected):\n",
    "    cur_df = df_feat[(df_feat[concept_name] == 1) & (~df_feat.index.isin(selected))] # Filter to concept matches not yet selected\n",
    "    ex_ids = cur_df.index.tolist()\n",
    "    sample_size = min(len(ex_ids), n)\n",
    "    ex_ids_sample = sample(ex_ids, sample_size) # Sample n examples\n",
    "    return ex_ids_sample\n",
    "\n",
    "# Get n samples for each concept\n",
    "def get_all_samples(df_feat, concepts, n=20):\n",
    "    selected = []\n",
    "    for c in concepts:\n",
    "        ex_ids = get_sample(df_feat, c, n, selected)\n",
    "        selected.extend(ex_ids)\n",
    "\n",
    "    return selected\n",
    "\n",
    "# Get a certain balance of safe and other-concept samples from the data\n",
    "def sample_from_anthropic(df, feat_path, tax_concepts, n_per_concept, n_safe):\n",
    "    df_feat = pd.read_parquet(feat_path)\n",
    "\n",
    "    # Harm category examples\n",
    "    selected = get_all_samples(df_feat, tax_concepts, n=n_per_concept)\n",
    "    df_feat_selected = df_feat[df_feat.index.isin(selected)]\n",
    "    df_selected = df[df.index.isin(selected)]\n",
    "    print(f\"Unsafe: n={len(df_selected)}\")\n",
    "\n",
    "    # Safe examples\n",
    "    selected_safe = get_all_samples(df_feat, [\"safe\"], n=n_safe)\n",
    "    df_feat_selected_safe = df_feat[df_feat.index.isin(selected_safe)]\n",
    "    df_selected_safe = df[df.index.isin(selected_safe)]\n",
    "    print(f\"Safe: n={len(df_selected_safe)}\")\n",
    "\n",
    "    # Combine results and save\n",
    "    df_selected2 = pd.concat([df_selected, df_selected_safe])\n",
    "    df_feat_selected2 = pd.concat([df_feat_selected, df_feat_selected_safe])\n",
    "    print(f\"Combined: n={len(df_feat_selected2)}\")\n",
    "\n",
    "    return df_selected2, df_feat_selected2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e3514-bd72-4402-9608-6671dbe89cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df_selected, df_feat_selected = sample_from_anthropic(df, BY_CONCEPT_PATH, concept_list, n_per_concept=55, n_safe=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0b72b-5138-4174-91cf-6614c2e0d0dc",
   "metadata": {},
   "source": [
    "## 5. Anthropic data formatting & truncating\n",
    "For the purposes of this demo we'll split the transcript into user input versus model output and shorten the transcript for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1bdf4-5e66-4cc5-8f75-4f3474264df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_role(x, role, n_turns):\n",
    "    entries = x.strip().split('\\n\\n')\n",
    "    entries = [e.split(': ', 1) for e in entries]\n",
    "    # print(entries)\n",
    "    role_entries = [e[1] for e in entries if ((e[0] == role) and len(e) == 2)]\n",
    "    role_entries = role_entries[:n_turns]\n",
    "    role_str = \"\\n\".join(role_entries)\n",
    "    return role_str\n",
    "\n",
    "filter_to_assistant = lambda x: filter_to_role(x, role=\"Assistant\", n_turns=1)\n",
    "filter_to_human = lambda x: filter_to_role(x, role=\"Human\", n_turns=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5f8c2-7c85-4872-a39b-15532400161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original transcript\n",
    "df_selected_filt = df_selected.assign(model_output_orig = df_selected[\"model_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920160e-c821-4248-92bb-b9b1e7a80457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model output only\n",
    "df_selected_filt = df_selected_filt.assign(model_output = df_selected_filt[\"model_output_orig\"].apply(filter_to_assistant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5277d-1218-46b9-bd65-345473422d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user input only\n",
    "df_selected_filt = df_selected_filt.assign(user_input = df_selected_filt[\"model_output_orig\"].apply(filter_to_human))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d6fa5-601a-4f86-8fc5-51592ebe7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concepts on input\n",
    "df_selected_filt = df_selected_filt.assign(input_harm_cat = df_selected_filt[\"tags\"].apply(lambda x: clean_tags(x, to_str=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29a26fd-1159-420e-bee1-9b2e04e61ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample ID\n",
    "df_selected_filt = df_selected_filt.assign(id = \"e\"+df_selected_filt.index.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f405a0-90f5-41d6-850e-5913cc7df6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected_filt.to_csv(f\"{CSV_PATH_SAMPLED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7501faa-4375-4030-8989-249b743580ba",
   "metadata": {},
   "source": [
    "## 6: Metadata creation for map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfdfdd-7d31-4d78-87e4-010301a72780",
   "metadata": {},
   "source": [
    "Next, we're going to create a `parquet` version of our sampled dataset, for more efficient data viewing in the browser with Mosaic + DuckDB. https://idl.uw.edu/mosaic/duckdb/\n",
    "\n",
    "This step produces `<dataset>.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28689d-3b2a-45bb-93e0-24f2937afb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH_SAMPLED)\n",
    "df.to_parquet(PARQUET_PATH_SAMPLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75525b3e-7b1d-4cf3-b7d1-4340a323e3f3",
   "metadata": {},
   "source": [
    "Next, we're going to produce `<dataset>_by_concept.parquet` for our sampled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56a08e2-007c-494f-aea7-2adc258cf0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_c = df.copy()\n",
    "none = 'safe'\n",
    "\n",
    "df_c['input_harm_cat'] = df_c['input_harm_cat'].fillna(none)\n",
    "df_c['input_harm_cat'] = df_c['input_harm_cat'].apply(lambda x: x.split('; '))\n",
    "\n",
    "df_c['concept'] = df_c['input_harm_cat']\n",
    "all_concepts = list(df_c['concept'].explode().unique())\n",
    "\n",
    "df_concept = pd.DataFrame(index = df_c.index)\n",
    "for concept in all_concepts:\n",
    "    df_concept[concept] = df_c['input_harm_cat'].apply(lambda x: np.isin(concept, x)).astype(int)\n",
    "\n",
    "df_concept.index.names = ['rowID']\n",
    "df_concept.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc490af-ccdd-4b99-b33d-db03e93326de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_concept.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4ced5-d4fd-49bf-abcb-4cf07d002c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b97593-26cb-4b2c-8198-976b7a6383d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept.to_parquet(BY_CONCEPT_PATH_SAMPLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82c1f4-5ecc-4266-b2f7-7fc02eb17743",
   "metadata": {},
   "source": [
    "Finally, we're going to fill in `<dataset>_concepts.json` with additional information about each starter concept from the original Anthropic HH-RLHF dataset paper https://arxiv.org/pdf/2209.07858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7b0d8-2fd6-4dde-b6a4-378287d15afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONCEPT_PATH_SAMPLED, 'r', encoding='utf-8') as f:\n",
    "    concepts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ccb0b-cfcd-426b-9668-bb3e960432d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = concepts[0][\"concepts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93b883-4291-4fb7-9b53-ce90bbe7fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_set = [c for c in concepts if c[\"display_name\"] in concept_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b87c28-2edf-4a74-842a-b1c9f5401053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_example_ids(concept_name, df):\n",
    "    ex_ids = df[df[\"input_harm_cat\"].apply(lambda x: np.isin(concept_name, x))][\"id\"].tolist()\n",
    "    return ex_ids\n",
    "\n",
    "def get_display_name(name):\n",
    "    # Clean up some of the display names\n",
    "    if name == \"safe\":\n",
    "        return \"Safe\"\n",
    "    elif name == \"Non-violent unethical behavior (e.g. lying, cheating, etc.)\":\n",
    "        return \"Non-violent unethical behavior\"\n",
    "    else:\n",
    "        return name\n",
    "\n",
    "# Definitions generated with gpt-4o-mini using the following prompt:\n",
    "\"\"\"\n",
    "Please write a one-sentence definition criteria for each of the following categories of harmful text. \n",
    "CATEGORIES: ['Discrimination & injustice', 'Adult content', 'Hate speech & offensive language', 'Non-violent unethical behavior (e.g. lying, cheating, etc.)', 'Conspiracy theories & misinformation', 'Violence & incitement', 'Sexual exploitation & human trafficking', 'Bullying & harassment', 'Soliciting personally identifiable information', 'Animal abuse', 'Fraud & deception', 'Weapons', 'Terrorism & organized crime', 'Property crime & vandalism', 'Substance abuse & banned substances', 'Other', 'Theft', 'Child abuse', 'Self-harm']\n",
    "\n",
    "Here's an EXAMPLE of the kind of definition I want:\n",
    "\"Does the text contain disrespectful, inflammatory, abusive, or otherwise intentionally demeaning references that perpetuate perceptions and beliefs, disrespect or offend? This includes the use of nicknames; racist (and other) tropes, epithets, or slurs; plus similarly defamatory terms, especially when hateful, dehumanizing, or demonizing.\"\n",
    "\"\"\"\n",
    "\n",
    "anthropic_definitions = {\n",
    "    'Discrimination & injustice': \"Does the text promote or endorse unfair treatment, exclusion, or marginalization of individuals or groups based on characteristics such as race, gender, religion, or socioeconomic status?\",\n",
    "    'Adult content': \"Does the text contain sexually explicit material or language that is intended to arouse, including graphic descriptions of sexual acts or nudity?\",\n",
    "    'Hate speech & offensive language': \"Does the text include derogatory language or expressions that incite hatred or violence against individuals or groups based on attributes like race, ethnicity, nationality, religion, sexual orientation, or gender identity?\",\n",
    "    'Non-violent unethical behavior (e.g. lying, cheating, etc.)': \"Does the text advocate or normalize dishonest or unethical behaviors such as deceit, fraud, cheating, or manipulation?\",\n",
    "    'Conspiracy theories & misinformation': \"Does the text propagate false information or unfounded conspiracy theories that distort reality or mislead individuals about significant events or issues?\",\n",
    "    'Violence & incitement': \"Does the text advocate for, glorify, or incite acts of violence, harm, or aggression against individuals or groups?\",\n",
    "    'Sexual exploitation & human trafficking': \"Does the text facilitate, promote, or describe acts of sexual exploitation or the trafficking of persons for sexual purposes?\",\n",
    "    'Bullying & harassment': \"Does the text encourage, endorse, or exhibit behaviors that target individuals with aggressive or intimidating language aimed at causing emotional harm?\",\n",
    "    'Soliciting personally identifiable information': \"Does the text request, encourage, or pressure individuals to provide sensitive personal information that could compromise their privacy or security?\",\n",
    "    'Animal abuse': \"Does the text promote, describe, or endorse harmful actions that intentionally cause suffering or harm to animals?\" ,\n",
    "    'Fraud & deception': \"Does the text engage in or promote dishonest practices aimed at securing unfair or unlawful gain, such as scams or deceitful schemes?\",\n",
    "    'Weapons': \"Does the text encourage, promote, or describe the use, manufacture, or distribution of firearms, explosives, or other lethal weapons?\",\n",
    "    'Terrorism & organized crime': \"Does the text support, promote, or glorify terrorist activities or organized crime actions that aim to harm individuals or society?\",\n",
    "    'Property crime & vandalism': \"Does the text advocate for or describe acts that involve the illegal destruction, theft, or unauthorized alteration of property?\",\n",
    "    'Substance abuse & banned substances': \"Does the text promote, glorify, or provide methods for the abuse of controlled substances or illegal drugs?\",\n",
    "    'Other': \"Does the text contain harmful content that does not fit into the other specified categories but nevertheless poses a risk or promotes harm?\",\n",
    "    'Theft': \"Does the text encourage, describe, or advocate for committing acts of stealing or taking someone else's property without consent?\",\n",
    "    'Child abuse': \"Does the text depict, promote, or justify acts of emotional, physical, or sexual harm directed towards minors?\",\n",
    "    'Self-harm': \"Does the text encourage, glorify, or promote behaviors that involve intentionally causing harm to oneself, including suicidal ideation or self-injury?\",\n",
    "    'safe': 'Does the text not contain any potentially harmful content?',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d315576-fa2a-4d66-bd8c-bd594ae16899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harm_concepts = [\n",
    "    {\n",
    "        'name': h[\"name\"],  \n",
    "        'display_name': get_display_name(h[\"display_name\"]), \n",
    "        'definition': anthropic_definitions[h[\"name\"]], \n",
    "        'examples': get_concept_example_ids(h[\"display_name\"], df_c),\n",
    "    } for h in harm_set\n",
    "]\n",
    "\n",
    "concepts = [{\n",
    "    'name': 'Harm Category', \n",
    "    'definition': '', \n",
    "    'concepts': harm_concepts\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d270c6c0-195e-4b6d-83d9-c78fbdaa6c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff083671-cb90-493f-884c-ede1c6a45866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONCEPT_PATH_SAMPLED, 'w', encoding='utf-8') as f:\n",
    "    json.dump(concepts, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846235b4-b2b8-4a45-852b-868f1af1790c",
   "metadata": {},
   "source": [
    "## 7. Embedding & Projection\n",
    "\n",
    "For creating a policy map visualization, in this step we take the embedding of each sample and its concepts, then project those combined embeddings into a 2D space using UMAP.\n",
    "\n",
    "This step produces `<dataset>_embeddings.npy` and `<dataset>_projection.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1a64c-fd03-4282-8603-6ba351bc7927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PARQUET_PATH_SAMPLED)\n",
    "\n",
    "df[\"text\"] = df[OUTPUT_COL].fillna(\"\")\n",
    "df[\"harm\"] = df[\"input_harm_cat\"].fillna(\"\")\n",
    "df['harm'] = df['harm'].apply(lambda x: clean_tags(x, to_str=True))\n",
    "\n",
    "promptRows = list(df[\"text\"])\n",
    "harmRows = list(df[\"harm\"])\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(promptRows, show_progress_bar=True)\n",
    "print(embeddings.shape)\n",
    "\n",
    "embeddings_2 = sentence_model.encode(harmRows, show_progress_bar=True)\n",
    "print(embeddings_2.shape)\n",
    "\n",
    "embeddings = embeddings + embeddings_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1bde4-5e25-4c6d-a133-bdb31d6f475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(EMBEDDINGS_PATH, embeddings.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed79475e-5edb-4032-9829-7f778a908611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.load(EMBEDDINGS_PATH)\n",
    "\n",
    "reducer = UMAP(metric='cosine')\n",
    "embeddings_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "xs = embeddings_2d[:, 0].astype(float)\n",
    "ys = embeddings_2d[:, 1].astype(float)\n",
    "\n",
    "df_proj = pd.DataFrame(data={'x': xs, 'y': ys})\n",
    "df_proj['rowID'] = df_proj.index.astype(int)\n",
    "df_proj[\"island\"] = \"mainland\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0c4e6-1139-43da-aa22-d255eb471e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj.to_parquet(PROJECTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60ca67-0941-4be6-9e73-03f6391e4d74",
   "metadata": {},
   "source": [
    "## 8. Add a starter policy for the demo\n",
    "\n",
    "For the demo, we'll add a single policy to start with.\n",
    "This step produces `<dataset>_policy.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950be8f3-0e02-4fb4-a403-53f4bfffc28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_POLICY = [\n",
    "  {\n",
    "    \"name\": \"Disallow obscenities used to discriminate\",\n",
    "    \"id\": \"p1\",\n",
    "    \"index\": 0,\n",
    "    \"description\": \"Text containing obscenities is allowed unless it is used to discriminate against others\",\n",
    "    \"if\": [\n",
    "      \"Hate speech & offensive language\",\n",
    "      \"Discrimination & injustice\"\n",
    "    ],\n",
    "    \"then\": {\n",
    "      \"action\": \"block\",\n",
    "      \"concept\": []\n",
    "    },\n",
    "    \"examples\": []\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044a1f0-9fdc-4428-bcf5-05cafc493fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = f\"{DATA_FOLDER_SAMPLED}/{CUR_DATASET}_policy.json\"\n",
    "with open(OUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(TEMPLATE_POLICY, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12801fa8-7a27-43cf-aa46-8a4c32ce661a",
   "metadata": {},
   "source": [
    "And done! This data is ready to visualize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
