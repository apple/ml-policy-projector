{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07fb4c4-a2fc-40f6-989b-139b99d01d33",
   "metadata": {},
   "source": [
    "# üìΩÔ∏è Policy Projector\n",
    "_This notebook can be used as a tutorial or a template for working with Policy Projector on an ordinary computer CPU with (optional) OpenAI models. ü§ñ With a GPU you can alternatively run with local models of your choice: see the GPU version of this notebook._\n",
    "\n",
    "For licensing see accompanying `LICENSE` file.\n",
    "Copyright (C) 2025 Apple Inc. All Rights Reserved.\n",
    "\n",
    "üìΩÔ∏è If developing locally, first follow the `README` instructions in this repo on launching the vite server. The python backend in that server is required to render Policy Projector's Jupyter Notebook widget\n",
    "\n",
    "‚ö†Ô∏è **Content Warning**: This tutorial covers the same use case from our paper: AI _safety_ policy. We use a formatted version of [hh-rlhf dataset from Anthropic](https://huggingface.co/datasets/Anthropic/hh-rlhf) from Bai et al. 2022, which is an LLM safety dataset. **This data contains harmful, unethical, and upsetting material which may be triggering to some individuals**. Please proceed with caution and mindfullness to your own wellbeing. The dataset content does not reflect the views of Apple or the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efec75-5e2e-4052-9bfd-d0dcda3bca28",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "### 1.0 Prepare data\n",
    "Before running this notebook, run `preprocess_data.ipynb` to generate and preprocess the Anthropic hh-rlhf demo data.\n",
    "\n",
    "### 1.1 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21469f-b2b8-4f7e-b790-f7779eb9a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b7dd4-051c-406e-8d05-f4ae1fe0e906",
   "metadata": {},
   "source": [
    "### 1.2 OpenAI Key (Optional)\n",
    "üìΩÔ∏è Note: You can explore and run most Policy Projector operations in this notebook _without_ an OpenAI API key, but the **suggest_concept()** and **concept_classify** features do require OpenAI models to run. If available, add your API key in a file named `.env` at the top directory of this repository, with a single line that goes: \n",
    "```\n",
    "OPENAI_API_KEY = 'my-actual-api-key-here'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc800df0-dd75-40ac-9c85-53499004b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # Load API key from a .env file\n",
    "openai_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dbf00d-6934-4a10-b9c1-a96eda0a4485",
   "metadata": {},
   "source": [
    "### 1.3 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733dadbc-9d44-472e-a4aa-508653cd1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd261c78-727b-4d21-b7d5-bb29b61ccd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f28aa-a428-4704-9241-2cc75ae25cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/anthropic_1000/anthropic_1000.csv\"\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b879bb-35fb-42ea-a386-2a69765b9d77",
   "metadata": {},
   "source": [
    "To get a sense of the harm categories present in this dataset, we'll sample a few. Note there are many messages labeled \"safe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13912021-b88e-4a74-a94d-9e43f8166024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.input_harm_cat.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec45e1-e8fb-4b98-8463-d94ff0d93693",
   "metadata": {},
   "source": [
    "### 1.4 Load Policy Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f712d-7ff5-45c5-9ff8-c5bfa458dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../policy-projector/policy_projector/src\")\n",
    "from policy_projector import PolicyProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cceaf-e935-43ec-bec0-02cb06334898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance\n",
    "p = PolicyProjector(\n",
    "    df, id_col=\"id\", in_text_col=\"user_input\", out_text_col=\"model_output\", \n",
    "    concept_col=\"input_harm_cat\",\n",
    "    auto_populate_limit=30,\n",
    "    base_model_path=None,  # For CPU usage where we're not using an on-device LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1f7d4-b5c1-4932-8674-04bdf11015b6",
   "metadata": {},
   "source": [
    "## 2. Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa02344-08ed-452f-885c-25f9121bd50d",
   "metadata": {},
   "source": [
    "### 2.1 Explore Current Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cc906-d307-4696-b790-2129bf099d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore global view of current concepts\n",
    "# User can get a preview of different currently-existing concepts\n",
    "p.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718372d3-87d2-4cb8-9523-81fd1626425f",
   "metadata": {},
   "source": [
    "### 2.2 Automatic Concept Suggestion from Latent Unlabeled Harms\n",
    "In Concept Suggestion, Poliucy Projector looks for potential conceptual \"blind spots\" in the data by asking LLMs to identify harm concepts that are _not existing labels_ in the dataset.\n",
    "\n",
    "üìΩÔ∏è When `suggest_concepts` is run with `debug` set to `True`, the suggested concepts are a fixed set of cached examples and _not_ freshly computed based on the data. This is to help you get a sense of what suggested concepts and their metadata will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a773545-f845-4806-a671-6b6f3e3b99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "await p.suggest_concepts(limit=100, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59724bf3-c84b-4266-a9f9-4fe19cd24347",
   "metadata": {},
   "source": [
    "üìΩÔ∏è To run `suggest_concepts` with `debug` set to `False`, you will need to have an API key with OpenAI loaded at the start of this notebook. Policy Projector's concept suggestion uses:\n",
    "- The natural language toolkit **nltk**\n",
    "- The concept induction (research) toolkit **Lloom** at https://stanfordhci.github.io/lloom/\n",
    "- ChatGPT models **gpt-4o-mini**, **gpt-4o**, and **text-embedding-3-large**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05fd0-7fd1-4d12-af7e-a87f83291928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be run only the first time you run this notebook\n",
    "# This step downloads a Punkt Sentence Tokenizer from nltk\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e7eec-e7b3-4f12-b3ca-8fc01445711e",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set debug to False to run with ChatGPT, or True to use cached suggestions\n",
    "await p.suggest_concepts(limit=100, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300d80e-a97c-4f84-a8c4-c71d65910a77",
   "metadata": {},
   "source": [
    "### 2.3 Add a New Concept\n",
    "You can add a new concept as suggested by the concept suggestion, or _manually_ by writing your own concept name, description, and examples. This is handy if there is concept you, your team, or users have already identifed and you want to map that concept to identify matching examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01809025-c085-4743-96c0-9ad8a5a395a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_spec = {\n",
    "  \"name\": \"Dangerous Actions\",\n",
    "  \"description\": \"Does the text example involve actions that could cause harm or danger?\",\n",
    "  \"examples\": [\n",
    "    \"e31945\",\n",
    "    \"e33075\"\n",
    "  ],\n",
    "  \"fixes\": [],\n",
    "  \"existing_concept\": \"\"\n",
    "}\n",
    "p.add_concept(cur_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7983fe4-07db-405d-bb90-25b019ee661a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.view_concept(name=\"Dangerous Actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf5fe9-4ecc-4cb3-aaee-f6384963d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_spec = {\n",
    "  \"name\": \"Health Risks\",\n",
    "  \"description\": \"Does the text example discuss risks to health or well-being?\",\n",
    "  \"examples\": [\n",
    "    \"e9973\",\n",
    "    \"e11930\"\n",
    "  ],\n",
    "  \"fixes\": [],\n",
    "  \"existing_concept\": \"\"\n",
    "}\n",
    "c = p.add_concept(cur_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4845e-9dbc-4dda-9d8e-87ec8737744f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.view_concept(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb6f8a-bb31-4fe4-afbb-a937fa936005",
   "metadata": {},
   "source": [
    "### 2.4 Classify Data by Concept\n",
    "Policy Projector uses an LLM to classify for each item in the dataset whether or not the item matches your concept.\n",
    "\n",
    "üìΩÔ∏è Just like `suggest_concepts`, to run `classify` with `debug` set to `False`, you will need to have an API key with OpenAI loaded at the start of this notebook. If `classify` is run with `debug` set to `True` you will see some fixed pre-computed results that illustrate what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f72819-459b-4867-a123-95c7216657a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = p.get_concept(name=\"Bullying & harassment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9b79d-c4cb-46a0-8dc2-c12fbd26677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fc4f7-34a5-4e77-83cd-a858c68f8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50 = df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fc10c-e276-4fd6-a6ad-c01a5fdb1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await c.classify(df_50, col=\"model_output\", in_text_col=\"user_input\", id_col=\"id\", n=50, show_widget=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d039c6-fc04-4e88-b9eb-1d84f51547ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73776d-5a06-4de3-a935-e386f54e3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c184aee-faf1-467f-b40a-ef01efedfc32",
   "metadata": {},
   "source": [
    "### 2.5 Export a Concept as a Spec\n",
    "The `to_spec` operator can be used to output or save your concept as a JSON-formatted specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec241070-172d-4823-bc15-109ff0304569",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = p.get_concept(name=\"Bullying & harassment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da45f01-019f-4d2a-97bc-9521aacd653b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c.to_spec(include_examples=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e73813-c4d2-495c-b351-b2437696358e",
   "metadata": {},
   "source": [
    "## 3. Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dcd0f0-e398-413d-bd27-bf0b0c5a8bc0",
   "metadata": {},
   "source": [
    "### 3.1 Create a New Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a301c-d112-4087-a6a2-bef13b2b872a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set debug to False to run with ChatGPT, or True to use cached suggestions\n",
    "await p.suggest_concepts(limit=100, spec_to_show=\"policy\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a76c8-e64a-4233-b512-9afd6771d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make underlying concept first\n",
    "cur_spec = {\n",
    "  \"name\": \"Violence and Harm\",\n",
    "  \"description\": \"Does the text example describe acts of violence or physical harm?\",\n",
    "  \"examples\": [\n",
    "    \"e14352\",\n",
    "    \"e25585\"\n",
    "  ],\n",
    "  \"fixes\": [],\n",
    "  \"existing_concept\": \"\"\n",
    "}\n",
    "c = p.add_concept(cur_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31764db6-6d47-4c82-92c9-27e68034ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_spec = {\n",
    "  \"name\": \"Violence and Harm\",\n",
    "  \"description\": \"Does the text example describe acts of violence or physical harm?\",\n",
    "  \"if\": [\n",
    "    \"Violence and Harm\"\n",
    "  ],\n",
    "  \"examples\": [\n",
    "    \"e14352\",\n",
    "    \"e25585\"\n",
    "  ]\n",
    "}\n",
    "pol_1 = p.add_policy(cur_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465247d-0853-4a26-bb6f-14cc6c5b6ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.view_policy(pol_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d34610-06d8-4b49-ae05-8b4ccb4c4df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.view_policy(p_id=\"Violence and Harm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd167e-bc0e-4b94-90f1-f6428a01f14e",
   "metadata": {},
   "source": [
    "### 3.2 Test Model Output against the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb290c0d-79e4-4474-9453-062032737206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50 = df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01636f69-3549-4d93-bb6b-2499442924f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pol_1.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43980f02-8fc2-4435-b6ad-1e6e480b2664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set debug to False to run with ChatGPT, or True to use (inaccurate) cached results\n",
    "await pol_1.match(df_50, col=\"model_output\", in_text_col=\"user_input\", id_col=\"id\", n=50, show_widget=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad84ee0-08fd-4723-9e48-ab41dfd70122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.view_policy(p_id=\"Violence and Harm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e77fb-1cdb-42a7-a49f-e920e28746ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pol_1.examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693298d-9852-4728-a330-4d5694714ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.view_concept(name=\"Violence and Harm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194dafe1-7955-4e1e-880c-b593a3675748",
   "metadata": {},
   "source": [
    "### 3.3 Export a Policy as a Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d000af9-ad57-4a89-b792-fc6b7f8cccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_1.to_spec(include_examples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e44c7-6daa-4842-8bc7-a83e10e65b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
